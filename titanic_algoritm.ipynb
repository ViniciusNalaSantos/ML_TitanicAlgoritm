{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/viniciusnalasantos/titanicalgoritm?scriptVersionId=117970787\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"#  Machine Learning Model to Predict Survival in Titanic\n\n<img src='https://cdn-images-1.medium.com/max/800/0*88vxNBEneFur5knf' alt=\"Picture\" width=\"600\" height=\"400\" style=\"display: block; margin: 0 auto\">\n<br>\n  Kaggle is an online community platform for data scientists and machine learning enthusiasts. Kaggle allows users to publish datasets, use GPU-integrated notebooks, and compete with other data scientists to solve data science challenges. Initially, this online platform aims to help professionals and learners reach their goals in their data science journey with the powerful tools and resources it provides. Today (2023), there are over 10 million registered users on Kaggle.\n  \n  All users can dispute with each other to see who builds the more precise machine learning model in the competition. In this article, we will construct one for \"Titanic - Machine Learning From Disaster\" competition. The objective is very simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. I chose this contest because it is well-known as the \"Hello World!\" of data science area, so there wasn't a better option to do an article showing the machine learning basics.\n\n### Table of Contents\n  \n[Part One](#part_one)\n* [Introduction](#introduction)\n* [Understand the Problem](#understand_problem)\n* [Obtain Data](#obtain_data)\n* [Exploratory Data Analysis](#exploratory_data_analysis)\n    - [Data Dictionary](#data_dictionary)\n    - [Overview](#overview)\n    - [Percentage of Missing Values](#percentage_missing_values)\n    - [Statistic Distribution](#statistic_distribution)\n    - [Which groups of people have more chance to survive?](#question)\n* [Conclusion](#conclusion)\n\n[Part Two](#part_two)\n* [Prepare the dataset](#prepare_dataset)\n    - [Assembling the datasets of train and test](#assemble_datasets)\n    - [Select the features](#select_features)\n    - [Missing Values](#missing_values)\n    - [Prepare the variables for the model](#prepare_variables)\n    - [Recuperating the datasets of train and test](#recuperating)\n* [Building the Models](#build_models)\n    - [Logistic Regression Model](#logistic_regression_model)\n    - [Decision Tree Model](#decision_tree_model)\n* [Submitting the results to Kaggle](#submit_results)\n* [Bonus: Would you survive the Titanic?](#bonus)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"part_one\"></a>\n<h2><b> Part One </b></h2>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n## Introduction\n   Every project in data science area requires - before importing the dataset and starting coding - follow a few steps:\n   \n- Understand the problem\n- Obtain data\n- Explore the data\n- Prepare the dataset\n- Modeling\n- Evaluate\n\nWhen you plan the route that you will take, doing a flowchart or a checklist of the sequence of tasks that needs to be done, the way becomes easier to surpass.\n  \n#### Code from the article: [Machine Learning Model to Predict Survival in Titanic [Pt. 1]](https://medium.com/@viniciusnala/machine-learning-model-to-predict-survival-in-titanic-pt-1-b3681d1794fb)\n   \n----","metadata":{}},{"cell_type":"markdown","source":"<a id=\"understand_problem\"></a>\n## Understand the problem\n  Although the variable luck played an important role in the survival of some passengers, there were people more prone to survive than others. And this is our first task: understand why someones have more chance to survive than others. Then we build a Machine Learning Model to predict if people survived or not, based on the data given by Kaggle.\n\n  The complete description of the competition is available on Kaggle website: https://www.kaggle.com/c/titanic\n\n###  Now, let's have fun!","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:49:00.061918Z","iopub.execute_input":"2023-01-25T17:49:00.062349Z","iopub.status.idle":"2023-01-25T17:49:00.072641Z","shell.execute_reply.started":"2023-01-25T17:49:00.062315Z","shell.execute_reply":"2023-01-25T17:49:00.071018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"obtain_data\"></a>\n## Obtain data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:49:00.401178Z","iopub.execute_input":"2023-01-25T17:49:00.401588Z","iopub.status.idle":"2023-01-25T17:49:00.418007Z","shell.execute_reply.started":"2023-01-25T17:49:00.401557Z","shell.execute_reply":"2023-01-25T17:49:00.416719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the necessary data can be accessed on the [competition site](https://www.kaggle.com/competitions/titanic), it has been split into two groups:\n\nTraining set (train.csv) <br>\n - should be used to build the machine-learning model\n - provide which passenger survived or not\n\nTest set (test.csv) <br>\n - should be used to see how well your model performs on unseen data\n - do not provide if the passenger survived or not\n\nTo download the files it's necessary to be registered on Kaggle.","metadata":{"execution":{"iopub.status.busy":"2023-01-15T14:08:23.397793Z","iopub.execute_input":"2023-01-15T14:08:23.398117Z","iopub.status.idle":"2023-01-15T14:08:23.40717Z","shell.execute_reply.started":"2023-01-15T14:08:23.398093Z","shell.execute_reply":"2023-01-15T14:08:23.405451Z"}}},{"cell_type":"markdown","source":"<a id=\"exploratory_data_analysis\"></a>\n## Exploratory Data Analysis\n  Undoubtedly, this is the most important part of the project, here we will spend 70% - 80% of our time. The quality of our analysis is directly related to the performance of our model.\n\n  In this step, the objective is to <b>identify variables that inform most about the target variable</b>. A good way to do this is by discovering the correlation between the informative variables and the target variable.\n\n  First, let's start looking at all the variables and briefly think about how it is related to the problem:  ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"data_dictionary\"></a>\n### Data Dictionary\n- Passengerid: Each passenger has a unique id number, so it does not affect anything in our problem.\n\n- Survival: Informs 1 when the passenger survived and 0 when died (target variable).\n\n- Pclass: Informs the class of the passenger (1 = 1st, 2 = 2nd, 3 = 3rd). \n\n- Names: As Passengerid, it does not have a relation with Survival, once is a unique value for each one.\n\n- Sex: Informs if the person is a male or female.\n\n- Age: Tells the age of each passenger.\n\n- SibSp: Total of spouses and siblings aboard the ship.\n\n- Parch: Total of parents and children aboard the ship.\n\n- Ticket: The number of the ticket, each passenger has a unique value.\n\n- Fare: Price of the passage.\n\n- Cabin: The number of the cabin of each passenger.\n\n- Embarked: From what port each person embarked (C = Cherbourg, Q = Queenstown, S = Southampton). ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"overview\"></a>\n### Overview\nNow, it's time to get our hands on the code, let's begin looking at the characteristics of the data set:","metadata":{}},{"cell_type":"code","source":"# How many lines and columns\nprint(f'Lines: {train.shape[0]}, Columns: {train.shape[1]}')","metadata":{"execution":{"iopub.status.busy":"2023-01-17T00:58:21.48461Z","iopub.execute_input":"2023-01-17T00:58:21.484968Z","iopub.status.idle":"2023-01-17T00:58:21.491011Z","shell.execute_reply.started":"2023-01-17T00:58:21.484942Z","shell.execute_reply":"2023-01-17T00:58:21.489623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the first 5 lines\ndisplay(train.head())\n\n# Identify the type of each variable\nprint(train.dtypes)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T00:58:23.16666Z","iopub.execute_input":"2023-01-17T00:58:23.16701Z","iopub.status.idle":"2023-01-17T00:58:23.186105Z","shell.execute_reply.started":"2023-01-17T00:58:23.166979Z","shell.execute_reply":"2023-01-17T00:58:23.185428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"percentage_missing_values\"></a>\n### Percentage of missing values","metadata":{}},{"cell_type":"code","source":"# See the percentage of missing values\n(train.isnull().sum() / train.shape[0]).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T19:54:56.551302Z","iopub.execute_input":"2023-01-17T19:54:56.551724Z","iopub.status.idle":"2023-01-17T19:54:56.570869Z","shell.execute_reply.started":"2023-01-17T19:54:56.551691Z","shell.execute_reply":"2023-01-17T19:54:56.569465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cabin comes with more than 77% of missing values, is the variable with the highest percentage of missing values in the dataset, followed by Age with 19% of missing values.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"statistic_distribution\"></a>\n### Statistic Distribution","metadata":{}},{"cell_type":"code","source":"# Statistic distribution\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T19:56:47.064718Z","iopub.execute_input":"2023-01-17T19:56:47.065419Z","iopub.status.idle":"2023-01-17T19:56:47.114039Z","shell.execute_reply.started":"2023-01-17T19:56:47.065379Z","shell.execute_reply":"2023-01-17T19:56:47.112815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the method .describe() we can see the measures of central tendency and some information about the statistical distribution of the dataset.","metadata":{}},{"cell_type":"code","source":"# Plot histograms\ntrain.hist(figsize=(10, 8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T19:58:05.25808Z","iopub.execute_input":"2023-01-17T19:58:05.258569Z","iopub.status.idle":"2023-01-17T19:58:06.156476Z","shell.execute_reply.started":"2023-01-17T19:58:05.258532Z","shell.execute_reply":"2023-01-17T19:58:06.154988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting histograms is a better way to see the statistical distribution of the dataset. It is very useful because we can get some pieces of information with just a quick view, as we can see above:\n\n- The majority of people died\n- There are fewer people in the 2nd class than in the 1st class\n- The statistical distribution of the age of the people aboard the ship, which most are 20 years old","metadata":{}},{"cell_type":"markdown","source":"<a id=\"question\"></a>\n### Which groups of people have more chance to survive?\n  Now we can start identifying correlations between the variables. To do this, let's plot some graphs comparing the survivors with sex, class of the passenger, and place embarked:","metadata":{}},{"cell_type":"code","source":"# Plot graph of Survived vs Sex, Pclass and Embarked\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,4))\n\nsns.barplot(x='Sex', y='Survived', data=train, ax=ax1)\nsns.barplot(x='Pclass', y='Survived', data=train, ax=ax2)\nsns.barplot(x='Embarked', y='Survived', data=train, ax=ax3)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:01:28.588897Z","iopub.execute_input":"2023-01-17T20:01:28.590209Z","iopub.status.idle":"2023-01-17T20:01:29.339766Z","shell.execute_reply.started":"2023-01-17T20:01:28.590149Z","shell.execute_reply":"2023-01-17T20:01:29.338801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Look how interesting plot a graph can be; with it, the following information can be extracted:\n- Woman has more chance to survive than men\n- A highest class means a highest chance to survive\n- More survivors: Cherbourg(C) > Queenstown(Q) > Southampton(S)\n  \nLook how understanding a graph can be very insightful, isn't it wonderful? The process of reading a graph and see what this graph is showing is an essential skill if you aim to become a data scientist. This skill can be improved by studying data analysis.\n\nLet's see another example:","metadata":{}},{"cell_type":"code","source":"# See the influence of age on the probability of surviving\nage_survived = sns.FacetGrid(train, col='Survived', height=5)\nage_survived.map(sns.histplot, 'Age', kde=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:03:54.112006Z","iopub.execute_input":"2023-01-17T20:03:54.112598Z","iopub.status.idle":"2023-01-17T20:03:54.658061Z","shell.execute_reply.started":"2023-01-17T20:03:54.112554Z","shell.execute_reply":"2023-01-17T20:03:54.657128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the age distribution of the survivors, we can see very discretely that there is a peak in the right graph, between 0 and 5 years. Although the charts are very similar, a more attentive eye can see that kids have more chance to survive: \"Ladies and children first!\".\n\nA very interesting type of graph that is worth mentioning is the Scatter  Matriz of Pandas:","metadata":{}},{"cell_type":"code","source":"# Plotting Age vs Pclass\ncolumns = ['Parch', 'SibSp', 'Age', 'Pclass']\npd.plotting.scatter_matrix(train[columns], figsize=(15,10));","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:05:34.557901Z","iopub.execute_input":"2023-01-17T20:05:34.559013Z","iopub.status.idle":"2023-01-17T20:05:35.712867Z","shell.execute_reply.started":"2023-01-17T20:05:34.558953Z","shell.execute_reply":"2023-01-17T20:05:35.711357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  The advantage of this kind of graph is that it brings useful information. Exhibiting the histogram of each variable we can see how is the distribution of that variable, and the scatter graph of all variables shows the relationship between them. \n  \n  After analyzing it, we can notice that the older ones were concentrated in the 1° class, while the younger ones were concentrated in the 3° class. To see it was not clear, it is a very subtle distinction that requires a more acute eye.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\nfig.suptitle('Distribution: Age X Class', fontsize=15, y=1)\naxes[0].set_title('Age in each Class', fontsize=12)\naxes[1].set_title('Age in each Class grouped by Survived', fontsize=12)\n\nsns.boxplot(ax=axes[0], data=train, x='Age', y='Pclass', orient='h', width=0.65)\nsns.boxplot(ax=axes[1], data=train, x='Age', y='Pclass', hue='Survived', orient='h', width=0.65)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:10:21.874645Z","iopub.execute_input":"2023-01-17T20:10:21.87506Z","iopub.status.idle":"2023-01-17T20:10:22.349373Z","shell.execute_reply.started":"2023-01-17T20:10:21.875026Z","shell.execute_reply":"2023-01-17T20:10:22.348255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here on the left graph, it's better to see this distinction: in the 3º and 2º classes a person of 60 years is considered an outlier, whereas in the 1º class doesn't. On the right graph we can see the same graph, but grouped by Survived (0=died, 1=alive), and we can see that in all the classes, the older ones died while the younger ones stayed alive, which supports what was mentioned above: younger people are more prone to survive.\n\nFinally, let's look at the heatmap of the variables to see the correlation between them. ","metadata":{}},{"cell_type":"code","source":"# Plot a heatmap to compare the variables\nsns.heatmap(data=train.corr(), annot=True, cmap='coolwarm', vmax=1.0, linewidths=1, fmt='.2f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:11:31.907038Z","iopub.execute_input":"2023-01-17T20:11:31.90864Z","iopub.status.idle":"2023-01-17T20:11:32.395864Z","shell.execute_reply.started":"2023-01-17T20:11:31.908562Z","shell.execute_reply":"2023-01-17T20:11:32.394675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation can be described as how much a variable influences another variable, this value can be calculated and varies between -1 and 1. -1 is a perfect negative correlation, which means that one variable decreases as the other increase; 0 means no correlation at all; +1 is a perfect positive correlation, which means that one variable increases, the other increases too.\n\nFor now, it's only this in Part 1. In the next part, we will prepare the data frame, handling the missing values, and start modeling.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n## Conclusion\n  In this article, we understood the situation, comprehended the problem, plotted the main graphs, and learned how to identify relevant variables through graph visualization.\n  \n  Remember: neglecting this initial phase, going directly to the modeling, and choosing informative variables without any criteria - will result in poor performance.\n  \n  If you wanna succeed in your analysis, learn how to document everything, detailing to the maximum each stage in the notebook.\n  \n  Next part we will prepare the dataset for the machine learning model: handle the missing data, outliers, and categorical variables. Start modeling and, in the last, we will evaluate the performance of the model.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"part_two\"></a>\n<br>  \n<h2><b> Part Two </b></h2>\n\n---\n In this article, we will give continuity to the Exploratory Analysis started in Part 1 of this series. If you hadn't seen it, please read it, without reading [Part 1](https://medium.com/@viniciusnala/machine-learning-model-to-predict-survival-in-titanic-pt-1-b3681d1794fb) you won't be able to fully understand what we will do in Part 2.\n\n We understood the problem and visualized the data plotting graphs. Now we will treat the data to leave them processable for our two models of Machine Learning: Logistic Regression and Decision Tree.\n \n To facilitate your apprenticeship, the notebook is available on my GitHub and Kaggle. I recommend - after reading this - doing all my steps again on your own.\n \n#### Code from the article: [Machine Learning Model to Predict Survival in Titanic [Pt. 2]](https://medium.com/@viniciusnala/machine-learning-model-to-predict-survival-in-titanic-pt-2-d0a01c1106bd)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"prepare_dataset\"></a>\n## Prepare the dataset\n Until now, all we did was import the data, formulate hypotheses, start an exploratory analysis of the data, and visualize graphs and correlations between the variables that we judged relevant.\n \n A data science project is not rigorous and linear, we don't need to follow step by step strictly. But it is an interactive process where we come and go every time that is necessary.\n \n When we take a notebook of another person, we have the impression that it is well-structured and he went direct to the point. Nevertheless, before bringing a beautiful version, I came and went to the beginning many times.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"assemble_datasets\"></a>\n### Assembling the datasets of train and test\n A method that is highly recommended when you are preparing the dataset for a Machine Learning Model is to assemble the datasets of train and test, and separate them in the end.\n \n Sometimes we have to do feature engineering, create dummy variables, or codify the variables. Therefore, our model will be trained using this architecture, and the test data will follow this same structure.\n \n Hence, it's easier to do all these phases for a single DataFrame and separate them again in the end.","metadata":{}},{"cell_type":"code","source":"# Save the index of the datasets to recuperate posteriously\ntrain_idx = train.shape[0]\ntest_idx = test.shape[0]\n\n# Save PassengerId for submission in Kaggle\npassengerid = test['PassengerId']\n\n# Extract the column Survived from train dataset and delete it\ntarget = train['Survived']\ntrain.drop(axis=1, columns='Survived', inplace=True)\n\n# Concatenate train and test into a single dataframe\ndf_merged = pd.concat(objs=[train, test]).reset_index(drop=True)\n\nprint(\"df_merged.shape: ({} x {})\".format(df_merged.shape[0], df_merged.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:49:08.501361Z","iopub.execute_input":"2023-01-25T17:49:08.501807Z","iopub.status.idle":"2023-01-25T17:49:08.51848Z","shell.execute_reply.started":"2023-01-25T17:49:08.50177Z","shell.execute_reply":"2023-01-25T17:49:08.516984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"select_features\"></a>\n### Select the features\n As any dataset of the real world, you will always come across data that is not useful, and others that don't have significance in your model. Many times our judgment can be mistaken, but is part of your role, as a data scientist, to choose which features will be used for the Machine Learning Model. In our case, we will disregard the variables ['PassengerId', 'Name', 'Ticket', 'Cabin'], because they don't seem important apparently.","metadata":{}},{"cell_type":"code","source":"# Delete the columns [Passenger Id, Name, Ticket and Cabin]\ndf_merged.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ndf_merged.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:49:10.599158Z","iopub.execute_input":"2023-01-25T17:49:10.600472Z","iopub.status.idle":"2023-01-25T17:49:10.615579Z","shell.execute_reply.started":"2023-01-25T17:49:10.600427Z","shell.execute_reply":"2023-01-25T17:49:10.614204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we will maintain the following variables to be prepared: ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'].","metadata":{}},{"cell_type":"markdown","source":"<a id=\"missing_values\"></a>\n### Missing Values\n  Let's look at the missing values of each column and decide how we will treat these empty spaces. The most common ways of dealing with missing values are these:\n \n* Fill these values with other values (mean, median, mode…)\n* Delete all the line\n\nEach case is a different case, and it's your role as a data scientist to decide the best approach. Normally, it's not recommended to throw all the line away in virtue of a single blank space. Always as possible, we try to fill the spaces, and this is what we will do.","metadata":{}},{"cell_type":"code","source":"# See the null values\nprint(df_merged.isnull().sum())\nprint(df_merged.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:49:13.262908Z","iopub.execute_input":"2023-01-25T17:49:13.263518Z","iopub.status.idle":"2023-01-25T17:49:13.272952Z","shell.execute_reply.started":"2023-01-25T17:49:13.263483Z","shell.execute_reply":"2023-01-25T17:49:13.271228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the variables age and fare, I will fill with the median; and for the variable Embarked, I will put the value that appears more frequently.","metadata":{}},{"cell_type":"code","source":"# Age\ndf_merged['Age'].fillna(df_merged['Age'].median(), inplace=True)\n\n# Fare\ndf_merged['Fare'].fillna(df_merged['Fare'].median(), inplace=True)\n\n# Embarked\ndf_merged['Embarked'].fillna(df_merged['Embarked'].mode(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:49:15.151847Z","iopub.execute_input":"2023-01-25T17:49:15.152246Z","iopub.status.idle":"2023-01-25T17:49:15.163403Z","shell.execute_reply.started":"2023-01-25T17:49:15.152215Z","shell.execute_reply":"2023-01-25T17:49:15.162172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"prepare_variables\"></a>\n### Prepare the variables for the model\n A mathematical model requires that we work with numerical values, therefore, we will have to convert the categorical values (text) to a number. As you can see below, I converted the values of Sex to {'male' : 0, 'female': 1}; regarding the Embarked variables, I applied the concept of dummy variables. The dummy variables only can have 0 or 1 as a value, creating a new column for each new variable. To make it easier to understand, look at how the DataFrame was after this treatment.","metadata":{}},{"cell_type":"code","source":"# Change the Sex column (male: 0, female: 1)\ndf_merged['Sex'] = df_merged['Sex'].map({'male': 0, 'female': 1})\n\n# Transform the Embarked column into 3 columns\nembarked_dummies = pd.get_dummies(df_merged['Embarked'], prefix='Embarked')\ndf_merged = pd.concat([df_merged, embarked_dummies], axis=1)\n\n# Drop the old Embarked column\ndf_merged.drop(axis=1, columns='Embarked', inplace=True)\n\ndisplay(df_merged.head())","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:49:18.337063Z","iopub.execute_input":"2023-01-25T17:49:18.3375Z","iopub.status.idle":"2023-01-25T17:49:18.361335Z","shell.execute_reply.started":"2023-01-25T17:49:18.337465Z","shell.execute_reply":"2023-01-25T17:49:18.36018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"recuperating\"></a>\n### Recuperating the datasets of train and test\n Now that we have prepared the dataset for the model, I will divide df_merged in train and test, exactly how it was in the beginning. Now you can understand the concatenation that we did before.","metadata":{}},{"cell_type":"code","source":"# Separating df_merged to train and test\ntrain = df_merged.iloc[:train_idx]\ntest = df_merged.iloc[train_idx:]","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:51:22.372725Z","iopub.execute_input":"2023-01-25T17:51:22.373244Z","iopub.status.idle":"2023-01-25T17:51:22.379356Z","shell.execute_reply.started":"2023-01-25T17:51:22.373201Z","shell.execute_reply":"2023-01-25T17:51:22.378281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"build_models\"></a>\n## Building the Models\n As I said in part one of this series, the stage that takes more time in every Data Science project is the Problem Comprehension and Data Exploratory Analysis.\n \n All the caution that we took to understand the problem, test hypotheses, and discard unnecessary/redundant variables. Will be used to build a basic Machine Learning Model, in this project we will build two:","metadata":{}},{"cell_type":"code","source":"# Import the libraries of the Machine Learning Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2023-01-25T17:52:36.931874Z","iopub.execute_input":"2023-01-25T17:52:36.932358Z","iopub.status.idle":"2023-01-25T17:52:37.260081Z","shell.execute_reply.started":"2023-01-25T17:52:36.932321Z","shell.execute_reply":"2023-01-25T17:52:37.259107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"logistic_regression_model\"></a>\n### Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"# Create a model of logistic regression\nlr_model = LogisticRegression(solver='liblinear')\nlr_model.fit(train, target)\n\n# Verifying the model accuracy\nacc_logReg = round(lr_model.score(train, target) * 100, 2)\nprint(\"The Logistic Regression Model has an accuracy of: {}%\".format(acc_logReg))","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:04:13.236948Z","iopub.execute_input":"2023-01-25T18:04:13.237395Z","iopub.status.idle":"2023-01-25T18:04:13.256226Z","shell.execute_reply.started":"2023-01-25T18:04:13.237356Z","shell.execute_reply":"2023-01-25T18:04:13.25468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" *LogisticRegression()* can be understood as creating the model, and using the method *.fit()* on this object can be understood as training the model. With the *.score()* method, we verify the accuracy of the model on the train data, which is 80%.\n\n Actually, it doesn't mean that our model is highly precise and we can already use it in real-life problems. Once we trained the model with the train data, this result only shows how well this model fits the data and if it captured well the general characteristics of the dataset.\n \n To see a better indicator of accuracy we will use the test data, which we only saved to check the precision of our model. This data didn't influence the model, and we will use it after building the second model.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"decision_tree_model\"></a>\n### Decision Tree Model","metadata":{}},{"cell_type":"code","source":"# Create a model of decision tree\ntree_model = DecisionTreeClassifier(max_depth=3)\ntree_model.fit(train, target)\n\n# Verifying the model accuracy\nacc_tree = round(tree_model.score(train, target) * 100, 2)\nprint(\"The Decision Tree Model has an accuracy of: {}%\".format(acc_tree))","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:05:38.788166Z","iopub.execute_input":"2023-01-25T18:05:38.788663Z","iopub.status.idle":"2023-01-25T18:05:38.804202Z","shell.execute_reply.started":"2023-01-25T18:05:38.788622Z","shell.execute_reply":"2023-01-25T18:05:38.802828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" One characteristic of the Decision Tree Model is that it's more flexible, making it easier to fit the train data, but, as I said, doesn't mean that this model is better because it is almost 3% higher than the other.\n\n Each type of model has its own peculiarities, some models are more precise at solving a specific problem and others are better for other types of problems.\n \n Show how each model works behind the scenes would demand more than only one article, and this is not the aim here. For now, to understand the general idea, knowing this is enough.\n \n I am planning to write about more complex aspects of these models and many others. However, this is still a plan, let's continue with our train of thought.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"submit_results\"></a>\n## Submitting the results to Kaggle\n Using the method *.predict()* we basically tell the model to predict if a person survived or not, based on the test data:","metadata":{}},{"cell_type":"code","source":"# Test data\ntest.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:06:54.303821Z","iopub.execute_input":"2023-01-25T18:06:54.304239Z","iopub.status.idle":"2023-01-25T18:06:54.327085Z","shell.execute_reply.started":"2023-01-25T18:06:54.304206Z","shell.execute_reply":"2023-01-25T18:06:54.325546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" It is very common the performance of the test prediction to be inferior to the train prediction, once the model was 100% trained upon the train data.","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\ny_pred_lr = lr_model.predict(test)\n\nsubmission = pd.DataFrame({\n    \"PassengerId\": passengerid,\n    \"Survived\": y_pred_lr\n})\n\n# generating csv file\nsubmission.to_csv('./submission_lr.csv', index=False)\n\n# Decision Tree\ny_pred_tree = tree_model.predict(test)\n\nsubmission = pd.DataFrame({\n    \"PassengerId\": passengerid,\n    \"Survived\": y_pred_tree\n})\n\n# generating csv file\nsubmission.to_csv('./submission_tree.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:18:04.690892Z","iopub.execute_input":"2023-01-25T18:18:04.691389Z","iopub.status.idle":"2023-01-25T18:18:04.709422Z","shell.execute_reply.started":"2023-01-25T18:18:04.691346Z","shell.execute_reply":"2023-01-25T18:18:04.707895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  Along your data science career, you will increase your analysis engineering skills and will be able to develop more complex models. However, reach almost 80% of accuracy in your first challenge is commendable and motivating!Along your data science career, you will increase your analysis engineering skills and will be able to develop more complex models. However, reach almost 80% of accuracy in your first challenge is commendable and motivating!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"bonus\"></a>\n## Bonus: Would you survive the Titanic?\n Have you ever asked if you would survive the Titanic shipwreck? Now you can see the answer.\n \n Using the same Decision Tree model, I will predict with my personal information if I would survive or not. Putting myself as a 2º class passenger, 18 years old, traveling on my own, paying the mean price of the ticket, and having embarked at the Queenstown port.\n \n I will create a 2d list to store my personal data and pass it as an argument to the *.predict()* method.","metadata":{}},{"cell_type":"code","source":"personal_data = [[2, 0, 18.0, 0, 0, train['Fare'].mean(), 0, 1, 0]]\n\nprint(f'Vinicius: {tree_model.predict(personal_data)[0]}')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T18:26:05.836018Z","iopub.execute_input":"2023-01-25T18:26:05.836462Z","iopub.status.idle":"2023-01-25T18:26:05.846276Z","shell.execute_reply.started":"2023-01-25T18:26:05.836429Z","shell.execute_reply":"2023-01-25T18:26:05.844513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Unfortunately, according to my own model, I would die if I was aboard the Titanic.\n\n I hope you have enjoyed this series of articles, try to replicate what I did here on your computer, plotting other types of graphs and extracting new insights. The Titanic challenge is the best option for those willing to begin in data science, it introduces the superficial layer of this astounding and profound area.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/viniciusnalasantos/titanicalgoritm?scriptVersionId=117296618\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"#  Machine Learning Model to Predict Survival in Titanic\n\n<img src='https://cdn-images-1.medium.com/max/800/0*88vxNBEneFur5knf' alt=\"Picture\" width=\"600\" height=\"400\" style=\"display: block; margin: 0 auto\">\n<br>\n  Kaggle is an online community platform for data scientists and machine learning enthusiasts. Kaggle allows users to publish datasets, use GPU-integrated notebooks, and compete with other data scientists to solve data science challenges. Initially, this online platform aims to help professionals and learners reach their goals in their data science journey with the powerful tools and resources it provides. Today (2023), there are over 10 million registered users on Kaggle.\n  \n  All users can dispute with each other to see who builds the more precise machine learning model in the competition. In this article, we will construct one for \"Titanic - Machine Learning From Disaster\" competition. The objective is very simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck. I chose this contest because it is well-known as the \"Hello World!\" of data science area, so there wasn't a better option to do an article showing the machine learning basics.\n\n### Table of Contents\n  \n[Part One](#part_one)\n* [Introduction](#introduction)\n* [Understand the Problem](#understand_problem)\n* [Obtain Data](#obtain_data)\n* [Exploratory Data Analysis](#exploratory_data_analysis)\n    - [Data Dictionary](#data_dictionary)\n    - [Overview](#overview)\n    - [Percentage of Missing Values](#percentage_missing_values)\n    - [Statistic Distribution](#statistic_distribution)\n    - [Which groups of people have more chance to survive?](#question)\n* [Conclusion](#conclusion)\n\n[Part Two](#part_two)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"part_one\"></a>\n<h2><b> Part One </b></h2>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n## Introduction\n   Every project in data science area requires - before importing the dataset and starting coding - follow a few steps:\n   \n- Understand the problem\n- Obtain data\n- Explore the data\n- Prepare the dataset\n- Modeling\n- Evaluate\n\nWhen you plan the route that you will take, doing a flowchart or a checklist of the sequence of tasks that needs to be done, the way becomes easier to surpass.\n  \n#### Code from the article: [Machine Learning](https://medium.com/@viniciusnala/machine-learning-model-to-predict-survival-in-titanic-pt-1-b3681d1794fb)\n   \n----","metadata":{}},{"cell_type":"markdown","source":"<a id=\"understand_problem\"></a>\n## Understand the problem\n  Although the variable luck played an important role in the survival of some passengers, there were people more prone to survive than others. And this is our first task: understand why someones have more chance to survive than others. Then we build a Machine Learning Model to predict if people survived or not, based on the data given by Kaggle.\n\n  The complete description of the competition is available on Kaggle website: https://www.kaggle.com/c/titanic\n\n###  Now, let's have fun!","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-01-25T16:09:55.478889Z","iopub.execute_input":"2023-01-25T16:09:55.479306Z","iopub.status.idle":"2023-01-25T16:09:55.966991Z","shell.execute_reply.started":"2023-01-25T16:09:55.479192Z","shell.execute_reply":"2023-01-25T16:09:55.965949Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"obtain_data\"></a>\n## Obtain data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-25T16:09:58.211101Z","iopub.execute_input":"2023-01-25T16:09:58.211459Z","iopub.status.idle":"2023-01-25T16:09:58.2361Z","shell.execute_reply.started":"2023-01-25T16:09:58.21143Z","shell.execute_reply":"2023-01-25T16:09:58.235462Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"All the necessary data can be accessed on the [competition site](https://www.kaggle.com/competitions/titanic), it has been split into two groups:\n\nTraining set (train.csv) <br>\n - should be used to build the machine-learning model\n - provide which passenger survived or not\n\nTest set (test.csv) <br>\n - should be used to see how well your model performs on unseen data\n - do not provide if the passenger survived or not\n\nTo download the files it's necessary to be registered on Kaggle.","metadata":{"execution":{"iopub.status.busy":"2023-01-15T14:08:23.397793Z","iopub.execute_input":"2023-01-15T14:08:23.398117Z","iopub.status.idle":"2023-01-15T14:08:23.40717Z","shell.execute_reply.started":"2023-01-15T14:08:23.398093Z","shell.execute_reply":"2023-01-15T14:08:23.405451Z"}}},{"cell_type":"markdown","source":"<a id=\"exploratory_data_analysis\"></a>\n## Exploratory Data Analysis\n  Undoubtedly, this is the most important part of the project, here we will spend 70% - 80% of our time. The quality of our analysis is directly related to the performance of our model.\n\n  In this step, the objective is to <b>identify variables that inform most about the target variable</b>. A good way to do this is by discovering the correlation between the informative variables and the target variable.\n\n  First, let's start looking at all the variables and briefly think about how it is related to the problem:  ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"data_dictionary\"></a>\n### Data Dictionary\n- Passengerid: Each passenger has a unique id number, so it does not affect anything in our problem.\n\n- Survival: Informs 1 when the passenger survived and 0 when died (target variable).\n\n- Pclass: Informs the class of the passenger (1 = 1st, 2 = 2nd, 3 = 3rd). \n\n- Names: As Passengerid, it does not have a relation with Survival, once is a unique value for each one.\n\n- Sex: Informs if the person is a male or female.\n\n- Age: Tells the age of each passenger.\n\n- SibSp: Total of spouses and siblings aboard the ship.\n\n- Parch: Total of parents and children aboard the ship.\n\n- Ticket: The number of the ticket, each passenger has a unique value.\n\n- Fare: Price of the passage.\n\n- Cabin: The number of the cabin of each passenger.\n\n- Embarked: From what port each person embarked (C = Cherbourg, Q = Queenstown, S = Southampton). ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"overview\"></a>\n### Overview\nNow, it's time to get our hands on the code, let's begin looking at the characteristics of the data set:","metadata":{}},{"cell_type":"code","source":"# How many lines and columns\nprint(f'Lines: {train.shape[0]}, Columns: {train.shape[1]}')","metadata":{"execution":{"iopub.status.busy":"2023-01-17T00:58:21.48461Z","iopub.execute_input":"2023-01-17T00:58:21.484968Z","iopub.status.idle":"2023-01-17T00:58:21.491011Z","shell.execute_reply.started":"2023-01-17T00:58:21.484942Z","shell.execute_reply":"2023-01-17T00:58:21.489623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the first 5 lines\ndisplay(train.head())\n\n# Identify the type of each variable\nprint(train.dtypes)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T00:58:23.16666Z","iopub.execute_input":"2023-01-17T00:58:23.16701Z","iopub.status.idle":"2023-01-17T00:58:23.186105Z","shell.execute_reply.started":"2023-01-17T00:58:23.166979Z","shell.execute_reply":"2023-01-17T00:58:23.185428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"percentage_missing_values\"></a>\n### Percentage of missing values","metadata":{}},{"cell_type":"code","source":"# See the percentage of missing values\n(train.isnull().sum() / train.shape[0]).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T19:54:56.551302Z","iopub.execute_input":"2023-01-17T19:54:56.551724Z","iopub.status.idle":"2023-01-17T19:54:56.570869Z","shell.execute_reply.started":"2023-01-17T19:54:56.551691Z","shell.execute_reply":"2023-01-17T19:54:56.569465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cabin comes with more than 77% of missing values, is the variable with the highest percentage of missing values in the dataset, followed by Age with 19% of missing values.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"statistic_distribution\"></a>\n### Statistic Distribution","metadata":{}},{"cell_type":"code","source":"# Statistic distribution\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T19:56:47.064718Z","iopub.execute_input":"2023-01-17T19:56:47.065419Z","iopub.status.idle":"2023-01-17T19:56:47.114039Z","shell.execute_reply.started":"2023-01-17T19:56:47.065379Z","shell.execute_reply":"2023-01-17T19:56:47.112815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the method .describe() we can see the measures of central tendency and some information about the statistical distribution of the dataset.","metadata":{}},{"cell_type":"code","source":"# Plot histograms\ntrain.hist(figsize=(10, 8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T19:58:05.25808Z","iopub.execute_input":"2023-01-17T19:58:05.258569Z","iopub.status.idle":"2023-01-17T19:58:06.156476Z","shell.execute_reply.started":"2023-01-17T19:58:05.258532Z","shell.execute_reply":"2023-01-17T19:58:06.154988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting histograms is a better way to see the statistical distribution of the dataset. It is very useful because we can get some pieces of information with just a quick view, as we can see above:\n\n- The majority of people died\n- There are fewer people in the 2nd class than in the 1st class\n- The statistical distribution of the age of the people aboard the ship, which most are 20 years old","metadata":{}},{"cell_type":"markdown","source":"<a id=\"question\"></a>\n### Which groups of people have more chance to survive?\n  Now we can start identifying correlations between the variables. To do this, let's plot some graphs comparing the survivors with sex, class of the passenger, and place embarked:","metadata":{}},{"cell_type":"code","source":"# Plot graph of Survived vs Sex, Pclass and Embarked\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,4))\n\nsns.barplot(x='Sex', y='Survived', data=train, ax=ax1)\nsns.barplot(x='Pclass', y='Survived', data=train, ax=ax2)\nsns.barplot(x='Embarked', y='Survived', data=train, ax=ax3)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:01:28.588897Z","iopub.execute_input":"2023-01-17T20:01:28.590209Z","iopub.status.idle":"2023-01-17T20:01:29.339766Z","shell.execute_reply.started":"2023-01-17T20:01:28.590149Z","shell.execute_reply":"2023-01-17T20:01:29.338801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Look how interesting plot a graph can be; with it, the following information can be extracted:\n- Woman has more chance to survive than men\n- A highest class means a highest chance to survive\n- More survivors: Cherbourg(C) > Queenstown(Q) > Southampton(S)\n  \nLook how understanding a graph can be very insightful, isn't it wonderful? The process of reading a graph and see what this graph is showing is an essential skill if you aim to become a data scientist. This skill can be improved by studying data analysis.\n\nLet's see another example:","metadata":{}},{"cell_type":"code","source":"# See the influence of age on the probability of surviving\nage_survived = sns.FacetGrid(train, col='Survived', height=5)\nage_survived.map(sns.histplot, 'Age', kde=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:03:54.112006Z","iopub.execute_input":"2023-01-17T20:03:54.112598Z","iopub.status.idle":"2023-01-17T20:03:54.658061Z","shell.execute_reply.started":"2023-01-17T20:03:54.112554Z","shell.execute_reply":"2023-01-17T20:03:54.657128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the age distribution of the survivors, we can see very discretely that there is a peak in the right graph, between 0 and 5 years. Although the charts are very similar, a more attentive eye can see that kids have more chance to survive: \"Ladies and children first!\".\n\nA very interesting type of graph that is worth mentioning is the Scatter  Matriz of Pandas:","metadata":{}},{"cell_type":"code","source":"# Plotting Age vs Pclass\ncolumns = ['Parch', 'SibSp', 'Age', 'Pclass']\npd.plotting.scatter_matrix(train[columns], figsize=(15,10));","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:05:34.557901Z","iopub.execute_input":"2023-01-17T20:05:34.559013Z","iopub.status.idle":"2023-01-17T20:05:35.712867Z","shell.execute_reply.started":"2023-01-17T20:05:34.558953Z","shell.execute_reply":"2023-01-17T20:05:35.711357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  The advantage of this kind of graph is that it brings useful information. Exhibiting the histogram of each variable we can see how is the distribution of that variable, and the scatter graph of all variables shows the relationship between them. \n  \n  After analyzing it, we can notice that the older ones were concentrated in the 1° class, while the younger ones were concentrated in the 3° class. To see it was not clear, it is a very subtle distinction that requires a more acute eye.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\nfig.suptitle('Distribution: Age X Class', fontsize=15, y=1)\naxes[0].set_title('Age in each Class', fontsize=12)\naxes[1].set_title('Age in each Class grouped by Survived', fontsize=12)\n\nsns.boxplot(ax=axes[0], data=train, x='Age', y='Pclass', orient='h', width=0.65)\nsns.boxplot(ax=axes[1], data=train, x='Age', y='Pclass', hue='Survived', orient='h', width=0.65)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:10:21.874645Z","iopub.execute_input":"2023-01-17T20:10:21.87506Z","iopub.status.idle":"2023-01-17T20:10:22.349373Z","shell.execute_reply.started":"2023-01-17T20:10:21.875026Z","shell.execute_reply":"2023-01-17T20:10:22.348255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here on the left graph, it's better to see this distinction: in the 3º and 2º classes a person of 60 years is considered an outlier, whereas in the 1º class doesn't. On the right graph we can see the same graph, but grouped by Survived (0=died, 1=alive), and we can see that in all the classes, the older ones died while the younger ones stayed alive, which supports what was mentioned above: younger people are more prone to survive.\n\nFinally, let's look at the heatmap of the variables to see the correlation between them. ","metadata":{}},{"cell_type":"code","source":"# Plot a heatmap to compare the variables\nsns.heatmap(data=train.corr(), annot=True, cmap='coolwarm', vmax=1.0, linewidths=1, fmt='.2f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:11:31.907038Z","iopub.execute_input":"2023-01-17T20:11:31.90864Z","iopub.status.idle":"2023-01-17T20:11:32.395864Z","shell.execute_reply.started":"2023-01-17T20:11:31.908562Z","shell.execute_reply":"2023-01-17T20:11:32.394675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation can be described as how much a variable influences another variable, this value can be calculated and varies between -1 and 1. -1 is a perfect negative correlation, which means that one variable decreases as the other increase; 0 means no correlation at all; +1 is a perfect positive correlation, which means that one variable increases, the other increases too.\n\nFor now, it's only this in Part 1. In the next part, we will prepare the data frame, handling the missing values, and start modeling.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n## Conclusion\n  In this article, we understood the situation, comprehended the problem, plotted the main graphs, and learned how to identify relevant variables through graph visualization.\n  \n  Remember: neglecting this initial phase, going directly to the modeling, and choosing informative variables without any criteria - will result in poor performance.\n  \n  If you wanna succeed in your analysis, learn how to document everything, detailing to the maximum each stage in the notebook.\n  \n  Next part we will prepare the dataset for the machine learning model: handle the missing data, outliers, and categorical variables. Start modeling and, in the last, we will evaluate the performance of the model.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"part_two\"></a>\n<br>  \n<h2><b> Part Two </b></h2>\n\n---\n In this article, we will give continuity to the Exploratory Analysis started in Part 1 of this series. If you hadn't seen it, please read it, without reading [Part 1](https://medium.com/@viniciusnala/machine-learning-model-to-predict-survival-in-titanic-pt-1-b3681d1794fb) you won't be able to fully understand what we will do in Part 2.\n\n We understood the problem and visualized the data plotting graphs. Now we will treat the data to leave them processable for our two models of Machine Learning: Logistic Regression and Decision Tree.\n \n To facilitate your apprenticeship, the notebook is available on my GitHub and Kaggle. I recommend - after reading this - doing all my steps again on your own.","metadata":{}},{"cell_type":"markdown","source":"## Prepare the dataset\n Until now, all we did was import the data, formulate hypotheses, start an exploratory analysis of the data, and visualize graphs and correlations between the variables that we judged relevant.\n \n A data science project is not rigorous and linear, we don't need to follow step by step strictly. But it is an interactive process where we come and go every time that is necessary.\n \n When we take a notebook of another person, we have the impression that it is well-structured and he went direct to the point. Nevertheless, before bringing a beautiful version, I came and went to the beginning many times.","metadata":{}},{"cell_type":"markdown","source":"### Assembling the datasets of train and test\n A method that is highly recommended when you are preparing the dataset for a Machine Learning Model is to assemble the datasets of train and test, and separate them in the end.\n \n Sometimes we have to do feature engineering, create dummy variables, or codify the variables. Therefore, our model will be trained using this architecture, and the test data will follow this same structure.\n \n Hence, it's easier to do all these phases for a single DataFrame and separate them again in the end.","metadata":{}},{"cell_type":"code","source":"# Save the index of the datasets to recuperate posteriously\ntrain_idx = train.shape[0]\ntest_idx = test.shape[0]\n\n# Save PassengerId for submission in Kaggle\npassengerid = test['PassengerId']\n\n# Extract the column Survived from train dataset and delete it\ntarget = train['Survived']\ntrain.drop(axis=1, columns='Survived', inplace=True)\n\n# Concatenate train and test into a single dataframe\ndf_merged = pd.concat(objs=[train, test]).reset_index(drop=True)\n\nprint(\"df_merged.shape: ({} x {})\".format(df_merged.shape[0], df_merged.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2023-01-25T16:11:11.51069Z","iopub.execute_input":"2023-01-25T16:11:11.510997Z","iopub.status.idle":"2023-01-25T16:11:11.53273Z","shell.execute_reply.started":"2023-01-25T16:11:11.510971Z","shell.execute_reply":"2023-01-25T16:11:11.531783Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"df_merged.shape: (1309 x 11)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Select the features\n As any dataset of the real world, you will always come across data that is not useful, and others that don't have significance in your model. Many times our judgment can be mistaken, but is part of your role, as a data scientist, to choose which features will be used for the Machine Learning Model. In our case, we will disregard the variables ['PassengerId', 'Name', 'Ticket', 'Cabin'], because they don't seem important apparently.","metadata":{}},{"cell_type":"code","source":"# Delete the columns [Passenger Id, Name, Ticket and Cabin]\ndf_merged.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ndf_merged.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T16:13:20.415676Z","iopub.execute_input":"2023-01-25T16:13:20.416014Z","iopub.status.idle":"2023-01-25T16:13:20.436622Z","shell.execute_reply.started":"2023-01-25T16:13:20.415986Z","shell.execute_reply":"2023-01-25T16:13:20.435773Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n0       3    male  22.0      1      0   7.2500        S\n1       1  female  38.0      1      0  71.2833        C\n2       3  female  26.0      0      0   7.9250        S\n3       1  female  35.0      1      0  53.1000        S\n4       3    male  35.0      0      0   8.0500        S","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"So, we will maintain the following variables to be prepared: ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'].","metadata":{}},{"cell_type":"markdown","source":"### Missing Values\n Let's look at the missing values of each column and decide how we will treat these empty spaces. The most common ways of dealing with missing values are these:\n \n* Fill these values with other values (mean, median, mode…)\n* Delete all the line\n\nEach case is a different case, and it's your role as a data scientist to decide the best approach. Normally, it's not recommended to throw all the line away in virtue of a single blank space. Always as possible, we try to fill the spaces, and this is what we will do.","metadata":{}},{"cell_type":"code","source":"# See the null values\nprint(df_merged.isnull().sum())\nprint(df_merged.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T16:16:39.83892Z","iopub.execute_input":"2023-01-25T16:16:39.839268Z","iopub.status.idle":"2023-01-25T16:16:39.846119Z","shell.execute_reply.started":"2023-01-25T16:16:39.83924Z","shell.execute_reply":"2023-01-25T16:16:39.845012Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Pclass        0\nSex           0\nAge         263\nSibSp         0\nParch         0\nFare          1\nEmbarked      2\ndtype: int64\n(1309, 7)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For the variables age and fare, I will fill with the median; and for the variable Embarked, I will put the value that appears more frequently.","metadata":{}},{"cell_type":"code","source":"# Age\ndf_merged['Age'].fillna(df_merged['Age'].median(), inplace=True)\n\n# Fare\ndf_merged['Fare'].fillna(df_merged['Fare'].median(), inplace=True)\n\n# Embarked\ndf_merged['Embarked'].fillna(df_merged['Embarked'].mode(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T16:20:56.288081Z","iopub.execute_input":"2023-01-25T16:20:56.288439Z","iopub.status.idle":"2023-01-25T16:20:56.297263Z","shell.execute_reply.started":"2023-01-25T16:20:56.288413Z","shell.execute_reply":"2023-01-25T16:20:56.296441Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Prepare the variables to the model\n\n# The mathematical model works with numbers,so I need to transform the categorical variables to numerical\n\n# Change the Sex column (male: 0, female: 1)\ndf_merged['Sex'] = df_merged['Sex'].map({'male': 0, 'female': 1})\n\n# Change the Embarked column in 3 columns and put 0 or 1 to each column, applying the dummies variables\n# Link to understand better this concept: https://www.youtube.com/watch?v=rAF1zzdgfRg\nembarked_dummies = pd.get_dummies(df_merged['Embarked'], prefix='Embarked')\ndf_merged = pd.concat([df_merged, embarked_dummies], axis=1)\ndf_merged.drop(axis=1, columns='Embarked', inplace=True)\n\ndisplay(df_merged.head())","metadata":{"execution":{"iopub.status.busy":"2023-01-20T16:15:37.500391Z","iopub.execute_input":"2023-01-20T16:15:37.500943Z","iopub.status.idle":"2023-01-20T16:15:37.527372Z","shell.execute_reply.started":"2023-01-20T16:15:37.500902Z","shell.execute_reply":"2023-01-20T16:15:37.52567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separating to dataset Train and dataset Test\ntrain = df_merged.iloc[:train_idx]\ntest = df_merged.iloc[train_idx:]","metadata":{"execution":{"iopub.status.busy":"2023-01-20T16:15:48.045023Z","iopub.execute_input":"2023-01-20T16:15:48.045486Z","iopub.status.idle":"2023-01-20T16:15:48.052143Z","shell.execute_reply.started":"2023-01-20T16:15:48.045452Z","shell.execute_reply":"2023-01-20T16:15:48.050488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the machine learning model\n\n# Analyse the data is the part that spend more time\n\n# Importing the libraries of the Machine Learning Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2023-01-20T16:15:51.775322Z","iopub.execute_input":"2023-01-20T16:15:51.775846Z","iopub.status.idle":"2023-01-20T16:15:52.135875Z","shell.execute_reply.started":"2023-01-20T16:15:51.775807Z","shell.execute_reply":"2023-01-20T16:15:52.13434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a model of logistic regression\nlr_model = LogisticRegression(solver='liblinear')\nlr_model.fit(train, target)\n\n# Verifying the model accuracy\nacc_logReg = round(lr_model.score(train, target) * 100, 2)\nprint(\"Acurácia do modelo de Regressão Logística: {}\".format(acc_logReg))","metadata":{"execution":{"iopub.status.busy":"2023-01-20T16:15:56.934634Z","iopub.execute_input":"2023-01-20T16:15:56.935112Z","iopub.status.idle":"2023-01-20T16:15:56.960048Z","shell.execute_reply.started":"2023-01-20T16:15:56.935076Z","shell.execute_reply":"2023-01-20T16:15:56.958697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict in test\ny_pred_lr = lr_model.predict(test)\n\nsubmission = pd.DataFrame({\n    \"PassengerId\": passengerid,\n    \"Survived\": y_pred_lr\n})\n\n# generating csv file\nsubmission.to_csv('./submission_lr.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T13:29:20.014658Z","iopub.execute_input":"2023-01-15T13:29:20.015078Z","iopub.status.idle":"2023-01-15T13:29:20.027776Z","shell.execute_reply.started":"2023-01-15T13:29:20.015052Z","shell.execute_reply":"2023-01-15T13:29:20.025649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build another Machine Learning Model\n\n# Create a model of decision tree\ntree_model = DecisionTreeClassifier(max_depth=3)\ntree_model.fit(train, target)\n\n# Verifying the accuracy of the model\nacc_tree = round(tree_model.score(train, target) * 100, 2)\nprint(\"Acurácia do modelo de Árvore de Decisão: {}\".format(acc_tree))","metadata":{"execution":{"iopub.status.busy":"2023-01-20T16:16:04.696058Z","iopub.execute_input":"2023-01-20T16:16:04.696496Z","iopub.status.idle":"2023-01-20T16:16:04.717439Z","shell.execute_reply.started":"2023-01-20T16:16:04.696464Z","shell.execute_reply":"2023-01-20T16:16:04.715556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_tree = tree_model.predict(test)\n\nsubmission = pd.DataFrame({\n    \"PassengerId\": passengerid,\n    \"Survived\": y_pred_tree\n})\n\n# gerar arquivo csv\nsubmission.to_csv('./submission_tree.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"personal_data = [[2, 0, 18.0, 0, 0, train['Fare'].mean(), 0, 1, 0]]\n\ntree_model.predict(personal_data)[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-20T16:25:55.493955Z","iopub.execute_input":"2023-01-20T16:25:55.494376Z","iopub.status.idle":"2023-01-20T16:25:55.505688Z","shell.execute_reply.started":"2023-01-20T16:25:55.494336Z","shell.execute_reply":"2023-01-20T16:25:55.504014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}